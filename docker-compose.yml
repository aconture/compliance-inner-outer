version: '2.1'
services:
    redis:
        container_name: airflow-redis
        image: 'redis:5.0.5'
        depends_on:
            - statsd-exporter

    postgres:
        container_name: airflow-postgres
        image: postgres:9.6
        depends_on:
            - statsd-exporter
        logging:
            options:
                max-size: 10m
                max-file: "3"
        environment:
            - POSTGRES_USER=airflow
            - POSTGRES_PASSWORD=airflow
            - POSTGRES_DB=airflow
        #volumes:
        #    - airflow-postgres-data:/var/lib/postgresql/data

    webserver:
        container_name: airflow-webserver
        image: apache/airflow:1.10.12-python3.7
        restart: always
        depends_on:
            - postgres
            - redis
            - statsd-exporter
        environment:
            - LOAD_EX=n
            - EXECUTOR=Local
            - POSTGRES_USER=airflow
            - POSTGRES_PASSWORD=airflow
            - POSTGRES_DB=airflow
            - AIRFLOW__SCHEDULER__STATSD_ON=True
            - AIRFLOW__SCHEDULER__STATSD_HOST=statsd-exporter
            - AIRFLOW__SCHEDULER__STATSD_PORT=8125
            - AIRFLOW__SCHEDULER__STATSD_PREFIX=airflow
            - AIRFLOW__CORE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres:5432/airflow
            # - AIRFLOW__CELERY__RESULT_BACKEND=db+postgresql://airflow:airflow@postgres:5432/airflow
            - AIRFLOW__CORE__FERNET_KEY=pMrhjIcqUNHMYRk_ZOBmMptWR6o1DahCXCKn5lEMpzM=
            - AIRFLOW__CORE__EXECUTOR=LocalExecutor
            - AIRFLOW__CORE__AIRFLOW_HOME=/opt/airflow/
            - AIRFLOW__CORE__LOAD_EXAMPLES=False
            - AIRFLOW__CORE__LOAD_DEFAULT_CONNECTIONS=False
            - AIRFLOW__WEBSERVER__WORKERS=2
            - AIRFLOW__WEBSERVER__WORKER_REFRESH_INTERVAL=1800
        volumes:
            - ./dags:/opt/airflow/dags
            #- ./airflow:/usr/local/airflow/dags
            
        ports:
            - "8080:8080"
        command: bash -c "airflow initdb && airflow webserver"
        healthcheck:
            test: ["CMD-SHELL", "[ -f /opt/airflow/airflow-webserver.pid ]"]
            interval: 30s
            timeout: 30s
            retries: 3

    flower:
        container_name: airflow-flower
        image: apache/airflow:1.10.12-python3.7
        restart: always
        depends_on:
            - redis
            - statsd-exporter
        environment:
            - EXECUTOR=Local
        ports:
            - "5555:5555"
        command: flower

    scheduler:
        container_name: airflow-scheduler
        image: apache/airflow:1.10.12-python3.7
        restart: always
        depends_on:
            - statsd-exporter
            - webserver
        volumes:
            - ./dags:/opt/airflow/dags
            #- ./airflow:/usr/local/airflow/dags
        extra_hosts:
            dlcondisdb21: 10.76.32.55
            ulcondisdb3: 10.76.32.151
        environment:
            - LOAD_EX=n
            - EXECUTOR=Local
            - POSTGRES_USER=airflow
            - POSTGRES_PASSWORD=airflow
            - POSTGRES_DB=airflow
            - AIRFLOW__SCHEDULER__STATSD_ON=True
            - AIRFLOW__SCHEDULER__STATSD_HOST=statsd-exporter
            - AIRFLOW__SCHEDULER__STATSD_PORT=8125
            - AIRFLOW__SCHEDULER__STATSD_PREFIX=airflow
            - AIRFLOW__CORE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres:5432/airflow
            # - AIRFLOW__CELERY__RESULT_BACKEND=db+postgresql://airflow:airflow@postgres:5432/airflow
            - AIRFLOW__CORE__FERNET_KEY=pMrhjIcqUNHMYRk_ZOBmMptWR6o1DahCXCKn5lEMpzM=
            - AIRFLOW__CORE__EXECUTOR=LocalExecutor
            - AIRFLOW__CORE__AIRFLOW_HOME=/opt/airflow/
            - AIRFLOW__CORE__LOAD_EXAMPLES=False
            - AIRFLOW__CORE__LOAD_DEFAULT_CONNECTIONS=False
        command: bash -c "sleep 30 && airflow scheduler"
        #command: bash -c "apt-get update -yqq && apt-get upgrade -yqq"
        #command: bash -c "apt-get install -yqq --no-install-recommends iputils-ping openssh-server sshpass"
        #command: bash -c "apt-get purge --auto-remove -yqq $buildDeps && apt-get autoremove -yqq --purge && apt-get clean"
        #command: bash -c "pip install --upgrade pip"
        #command: bash -c "pip install ansible==2.9.11"
        #command: bash -c "pip install ansible-runner==1.4.6"
        #command: bash -c "pip install bottle==0.12.18 && pip install openpyxl==3.0.3"
        #command: bash -c "mkdir /etc/ansible && chmod +755 /etc/ansible"
        #command: bash -c "mkdir /etc/ansible && chmod +755 /etc/ansible"



    # worker:
    #     container_name: airflow-worker
    #     image: apache/airflow:1.10.12-python3.7
    #     restart: always
    #     depends_on:
    #         - statsd-exporter
    #         - postgres
    #         - webserver
    #     volumes:
    #         - ./dags:/opt/airflow/dags
    #     environment:
    #         - EXECUTOR=Local
    #         - POSTGRES_USER=airflow
    #         - POSTGRES_PASSWORD=airflow
    #         - POSTGRES_DB=airflow
    #         - AIRFLOW__SCHEDULER__STATSD_ON=True
    #         - AIRFLOW__SCHEDULER__STATSD_HOST=statsd-exporter
    #         - AIRFLOW__SCHEDULER__STATSD_PORT=8125
    #         - AIRFLOW__SCHEDULER__STATSD_PREFIX=airflow
    #         - AIRFLOW__CORE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres:5432/airflow
    #         # - AIRFLOW__CELERY__RESULT_BACKEND=db+postgresql://airflow:airflow@postgres:5432/airflow
    #         - AIRFLOW__CELERY__CELERY_RESULT_BACKEND=db+postgresql://airflow:airflow@postgres:5432/airflow
    #         - AIRFLOW__CORE__FERNET_KEY=pMrhjIcqUNHMYRk_ZOBmMptWR6o1DahCXCKn5lEMpzM=
    #         - AIRFLOW__CORE__EXECUTOR=LocalExecutor
    #         - AIRFLOW__CORE__AIRFLOW_HOME=/opt/airflow/
    #         - AIRFLOW__CORE__LOAD_EXAMPLES=False
    #         - AIRFLOW__CORE__LOAD_DEFAULT_CONNECTIONS=False
    #     command: bash -c "sleep 30 && airflow worker"

    statsd-exporter:
        image: prom/statsd-exporter
        container_name: airflow-statsd-exporter
        command: "--statsd.listen-udp=:8125 --web.listen-address=:9102"
        ports:
            - 9123:9102
            - 8125:8125/udp
    
    prometheus:
        image: prom/prometheus
        container_name: airflow-prometheus
        user: "0"
        ports:
            - 9090:9090
        volumes:
            - ./prometheus/prometheus.yml:/etc/prometheus/prometheus.yml
            - ./prometheus/volume:/prometheus
        
    grafana:
        image: grafana/grafana:7.1.5
        container_name: airflow-grafana
        environment:
            GF_SECURITY_ADMIN_USER: telecom
            GF_SECURITY_ADMIN_PASSWORD: telecom
            GF_PATHS_PROVISIONING: /grafana/provisioning
        ports:
            - 3000:3000
        volumes:
            - ./grafana/volume/data:/grafana
            - ./grafana/volume/datasources:/grafana/datasources
            - ./grafana/volume/dashboards:/grafana/dashboards
            - ./grafana/volume/provisioning:/grafana/provisioning 
        
    influxdb:
        image: influxdb
        container_name: influxdb
        restart: always
        ports:
            - 8086:8086
        volumes:
            - influxdb-volume:/vol01/Docker/monitoring
        environment:
            - INFLUXDB_DB=telegraf
            #- INFLUXDB_USER=telegraf
            #- INFLUXDB_ADMIN_ENABLED=true
            #- INFLUXDB_ADMIN_USER=admin
            #- INFLUXDB_ADMIN_PASSWORD=Welcome1 
    telegraf:
        image: telegraf
        container_name: telegraf
        restart: always
        extra_hosts:
            - "influxdb:127.0.0.1"
        environment:
            HOST_PROC: /rootfs/proc
            HOST_SYS: /rootfs/sys
            HOST_ETC: /rootfs/etc
        volumes:
            - ./telegraf.conf:/etc/telegraf/telegraf.conf:ro
            - /var/run/docker.sock:/var/run/docker.sock:ro
            - /sys:/rootfs/sys:ro
            - /proc:/rootfs/proc:ro
            - /etc:/rootfs/etc:ro
    
volumes:
    influxdb-volume:
        driver: local